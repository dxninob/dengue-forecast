{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53255bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\dxnin\\Documents\\dengue-forecast\n"
     ]
    }
   ],
   "source": [
    "from parameters import get_parameters\n",
    "from utils_sarimax import *\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import t\n",
    "\n",
    "params = get_parameters()\n",
    "source_model = params[\"source_model\"]\n",
    "source_results = params[\"source_results\"]\n",
    "validation_year = params[\"validation_year\"]\n",
    "test_year = params[\"test_year\"]\n",
    "\n",
    "\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83efb5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            CASES\n",
      "DATE             \n",
      "2007-05-13      0\n",
      "2007-05-20      0\n",
      "2007-05-27      0\n",
      "2007-06-03      0\n",
      "2007-06-10      0\n",
      "...           ...\n",
      "2023-11-26     12\n",
      "2023-12-03     10\n",
      "2023-12-10     13\n",
      "2023-12-17     14\n",
      "2023-12-24      7\n",
      "\n",
      "[868 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(f\"{PROJECT_ROOT}/{source_model}/data.pkl\")\n",
    "df = df[[\"CASES\"]]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ede7386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df, train_df, val_df, test_df = split_by_date(df, validation_year, test_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab2bd6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median validation: 7.0\n",
      "Mean validation: 8.426229508196721\n"
     ]
    }
   ],
   "source": [
    "print(\"Median validation:\", val_df[\"CASES\"].median())\n",
    "print(\"Mean validation:\", val_df[\"CASES\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a1a5433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median test: 3.0\n",
      "Mean test: 3.5355191256830603\n"
     ]
    }
   ],
   "source": [
    "print(\"Median test:\", test_df[\"CASES\"].median())\n",
    "print(\"Mean test:\", test_df[\"CASES\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade0974a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   DM statistic p-value (%)           Ganador\n",
      "SARIMAX validation    -0.792569    0.429061  No significativo\n",
      "SARIMAX test          -0.989632    0.323668  No significativo\n",
      "LSTM validation       -1.144836     0.25378  No significativo\n",
      "LSTM test             -0.135443    0.892411  No significativo\n"
     ]
    }
   ],
   "source": [
    "def diebold_mariano(e1, e2, h=1, loss=\"rmse\"):\n",
    "    e1, e2 = np.asarray(e1), np.asarray(e2)\n",
    "\n",
    "    if loss == \"rmse\":\n",
    "        d = np.abs(e1) - np.abs(e2)\n",
    "    elif loss == \"mse\":\n",
    "        d = e1**2 - e2**2\n",
    "    elif loss == \"mae\":\n",
    "        d = np.abs(e1) - np.abs(e2)\n",
    "    elif loss == \"mape\":\n",
    "        d = np.abs(e1 / (e1 + e2)) - np.abs(e2 / (e1 + e2))\n",
    "    else:\n",
    "        raise ValueError(\"loss debe ser 'mse', 'mae' o 'mape'\")\n",
    "\n",
    "    d_mean = np.mean(d)\n",
    "    T = len(d)\n",
    "\n",
    "    gamma = np.zeros(h)\n",
    "    for lag in range(h):\n",
    "        cov = np.sum((d[:T-lag] - d_mean) * (d[lag:] - d_mean))\n",
    "        gamma[lag] = cov / (T - lag)\n",
    "\n",
    "    V = gamma[0]\n",
    "    for lag in range(1, h):\n",
    "        weight = 1 - lag / h\n",
    "        V += 2 * weight * gamma[lag]\n",
    "\n",
    "    DM = d_mean / np.sqrt(V / T)\n",
    "    p_value = 2 * (1 - t.cdf(abs(DM), df=T - 1))\n",
    "\n",
    "    return DM, p_value\n",
    "\n",
    "def compare_models(df1, df2, model1_name, model2_name):\n",
    "    true1 = df1[\"Actual\"].values\n",
    "    pred1 = df1[\"Predicted\"].values\n",
    "    true2 = df2[\"Actual\"].values\n",
    "    pred2 = df2[\"Predicted\"].values\n",
    "    \n",
    "    if not np.allclose(true1, true2):\n",
    "        print(f\"Advertencia: {model1_name} y {model2_name} no tienen el mismo 'Actual'. Se usar√° el de df1.\")\n",
    "\n",
    "    e1 = true1 - pred1\n",
    "    e2 = true1 - pred2\n",
    "\n",
    "    DM, p = diebold_mariano(e1, e2, h=1, loss=\"rmse\")\n",
    "\n",
    "    if p < 0.05:\n",
    "        winner = model1_name if DM < 0 else model2_name\n",
    "    else:\n",
    "        winner = \"No significativo\"\n",
    "\n",
    "    return DM, p, winner\n",
    "\n",
    "\n",
    "def load_results(PROJECT_ROOT, source_results):\n",
    "    paths = {\n",
    "        \"LSTM_val\":      f\"{PROJECT_ROOT}/{source_results}/LSTM/test/true_pred.pkl\",\n",
    "        \"LSTM_exog_val\": f\"{PROJECT_ROOT}/{source_results}/LSTM_exog/test/true_pred.pkl\",\n",
    "        \"SARIMAX_val\":      f\"{PROJECT_ROOT}/{source_results}/SARIMAX/test/true_pred.pkl\",\n",
    "        \"SARIMAX_exog_val\": f\"{PROJECT_ROOT}/{source_results}/SARIMAX_exog/test/true_pred.pkl\",\n",
    "\n",
    "        \"LSTM_test\":      f\"{PROJECT_ROOT}/{source_results}/LSTM/validation/best_true_pred.pkl\",\n",
    "        \"LSTM_exog_test\": f\"{PROJECT_ROOT}/{source_results}/LSTM_exog/validation/best_true_pred.pkl\",\n",
    "        \"SARIMAX_test\":      f\"{PROJECT_ROOT}/{source_results}/SARIMAX/validation/best_true_pred.pkl\",\n",
    "        \"SARIMAX_exog_test\": f\"{PROJECT_ROOT}/{source_results}/SARIMAX_exog/validation/best_true_pred.pkl\"\n",
    "    }\n",
    "\n",
    "    dfs = {name: pd.read_pickle(path) for name, path in paths.items()}\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def run_dm_analysis(PROJECT_ROOT, source_results):\n",
    "\n",
    "    dfs = load_results(PROJECT_ROOT, source_results)\n",
    "\n",
    "    results = {\n",
    "        \"SARIMAX validation\": compare_models(\n",
    "            dfs[\"SARIMAX_val\"], dfs[\"SARIMAX_exog_val\"], \"SARIMAX\", \"SARIMAX_exog\"\n",
    "        ),\n",
    "        \"SARIMAX test\": compare_models(\n",
    "            dfs[\"SARIMAX_test\"], dfs[\"SARIMAX_exog_test\"], \"SARIMAX\", \"SARIMAX_exog\"\n",
    "        ),\n",
    "        \"LSTM validation\": compare_models(\n",
    "            dfs[\"LSTM_val\"], dfs[\"LSTM_exog_val\"], \"LSTM\", \"LSTM_exog\"\n",
    "        ),\n",
    "        \"LSTM test\": compare_models(\n",
    "            dfs[\"LSTM_test\"], dfs[\"LSTM_exog_test\"], \"LSTM\", \"LSTM_exog\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    table = pd.DataFrame({\n",
    "        col: {\n",
    "            \"DM statistic\": results[col][0],\n",
    "            \"p-value (%)\": results[col][1],\n",
    "            \"Ganador\": results[col][2]\n",
    "        }\n",
    "        for col in results\n",
    "    })\n",
    "\n",
    "    return table.T\n",
    "\n",
    "\n",
    "table = run_dm_analysis(PROJECT_ROOT, source_results)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a7dbe55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     models     dataset  DM statistic  p-value (%)  \\\n",
      "0          Naive vs SARIMAX  Validation     -0.089562     0.928734   \n",
      "1  Naive vs SARIMAX Climate  Validation     -0.337110     0.736422   \n",
      "2             Naive vs LSTM  Validation      3.350175     0.000982   \n",
      "3     Naive vs LSTM Climate  Validation      2.917147     0.003977   \n",
      "4          Naive vs SARIMAX        Test     -1.969385     0.050427   \n",
      "5  Naive vs SARIMAX Climate        Test     -2.202209     0.028907   \n",
      "6             Naive vs LSTM        Test      3.082556     0.002372   \n",
      "7     Naive vs LSTM Climate        Test      2.167237     0.031516   \n",
      "\n",
      "            Ganador  \n",
      "0  No significativo  \n",
      "1  No significativo  \n",
      "2              LSTM  \n",
      "3      LSTM Climate  \n",
      "4  No significativo  \n",
      "5             Naive  \n",
      "6              LSTM  \n",
      "7      LSTM Climate  \n"
     ]
    }
   ],
   "source": [
    "def compare_with_naive(naive_df, model_df, model_name, dataset_name):\n",
    "    true_n = naive_df[\"Actual\"].values\n",
    "    pred_n = naive_df[\"Predicted\"].values\n",
    "    \n",
    "    true_m = model_df[\"Actual\"].values\n",
    "    pred_m = model_df[\"Predicted\"].values\n",
    "\n",
    "    if not np.allclose(true_n, true_m):\n",
    "        print(f\"Advertencia: Naive y {model_name} no tienen el mismo 'Actual'. Se usa el de Naive.\")\n",
    "\n",
    "    e1 = true_n - pred_n  \n",
    "    e2 = true_n - pred_m  \n",
    "\n",
    "    DM, p = diebold_mariano(e1, e2, h=1, loss=\"rmse\")\n",
    "\n",
    "    if p < 0.05:\n",
    "        winner = \"Naive\" if DM < 0 else model_name\n",
    "    else:\n",
    "        winner = \"No significativo\"\n",
    "\n",
    "    return {\n",
    "        \"models\": f\"Naive vs {model_name}\",\n",
    "        \"dataset\": dataset_name,\n",
    "        \"DM statistic\": DM,\n",
    "        \"p-value (%)\": p,\n",
    "        \"Ganador\": winner\n",
    "    }\n",
    "\n",
    "\n",
    "def load_results(PROJECT_ROOT, source_results):\n",
    "    paths = {\n",
    "        \"LSTM_val\": f\"{PROJECT_ROOT}/{source_results}/LSTM/test/true_pred.pkl\",\n",
    "        \"LSTM_test\": f\"{PROJECT_ROOT}/{source_results}/LSTM/validation/best_true_pred.pkl\",\n",
    "\n",
    "        \"LSTM_exog_val\": f\"{PROJECT_ROOT}/{source_results}/LSTM_exog/test/true_pred.pkl\",\n",
    "        \"LSTM_exog_test\": f\"{PROJECT_ROOT}/{source_results}/LSTM_exog/validation/best_true_pred.pkl\",\n",
    "\n",
    "        \"SARIMAX_val\": f\"{PROJECT_ROOT}/{source_results}/SARIMAX/test/true_pred.pkl\",\n",
    "        \"SARIMAX_test\": f\"{PROJECT_ROOT}/{source_results}/SARIMAX/validation/best_true_pred.pkl\",\n",
    "\n",
    "        \"SARIMAX_exog_val\": f\"{PROJECT_ROOT}/{source_results}/SARIMAX_exog/test/true_pred.pkl\",\n",
    "        \"SARIMAX_exog_test\": f\"{PROJECT_ROOT}/{source_results}/SARIMAX_exog/validation/best_true_pred.pkl\",\n",
    "    }\n",
    "\n",
    "    dfs = {name: pd.read_pickle(path) for name, path in paths.items()}\n",
    "\n",
    "    actual_val = dfs[\"LSTM_val\"][\"Actual\"].copy().reset_index(drop=True)\n",
    "    naive_pred_val = actual_val.shift(1)\n",
    "    naive_pred_val.iloc[0] = actual_val.iloc[0]\n",
    "\n",
    "    dfs[\"Naive_val\"] = pd.DataFrame({\n",
    "        \"Actual\": actual_val,\n",
    "        \"Predicted\": naive_pred_val\n",
    "    })\n",
    "\n",
    "    actual_test = dfs[\"LSTM_test\"][\"Actual\"].copy().reset_index(drop=True)\n",
    "    naive_pred_test = actual_test.shift(1)\n",
    "    naive_pred_test.iloc[0] = actual_test.iloc[0]\n",
    "\n",
    "    dfs[\"Naive_test\"] = pd.DataFrame({\n",
    "        \"Actual\": actual_test,\n",
    "        \"Predicted\": naive_pred_test\n",
    "    })\n",
    "\n",
    "    return dfs\n",
    "\n",
    "\n",
    "\n",
    "def run_dm_naive(PROJECT_ROOT, source_results):\n",
    "\n",
    "    dfs = load_results(PROJECT_ROOT, source_results)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    rows.append(compare_with_naive(dfs[\"Naive_val\"], dfs[\"SARIMAX_val\"], \"SARIMAX\", \"Validation\"))\n",
    "    rows.append(compare_with_naive(dfs[\"Naive_val\"], dfs[\"SARIMAX_exog_val\"], \"SARIMAX Climate\", \"Validation\"))\n",
    "    rows.append(compare_with_naive(dfs[\"Naive_val\"], dfs[\"LSTM_val\"], \"LSTM\", \"Validation\"))\n",
    "    rows.append(compare_with_naive(dfs[\"Naive_val\"], dfs[\"LSTM_exog_val\"], \"LSTM Climate\", \"Validation\"))\n",
    "\n",
    "    rows.append(compare_with_naive(dfs[\"Naive_test\"], dfs[\"SARIMAX_test\"], \"SARIMAX\", \"Test\"))\n",
    "    rows.append(compare_with_naive(dfs[\"Naive_test\"], dfs[\"SARIMAX_exog_test\"], \"SARIMAX Climate\", \"Test\"))\n",
    "    rows.append(compare_with_naive(dfs[\"Naive_test\"], dfs[\"LSTM_test\"], \"LSTM\", \"Test\"))\n",
    "    rows.append(compare_with_naive(dfs[\"Naive_test\"], dfs[\"LSTM_exog_test\"], \"LSTM Climate\", \"Test\"))\n",
    "\n",
    "    table = pd.DataFrame(rows)\n",
    "    return table\n",
    "\n",
    "\n",
    "table = run_dm_naive(PROJECT_ROOT, source_results)\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
